{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be124204-17b0-4fb0-aae8-533286dc5e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 14:30:36.267570: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-27 14:30:36.278994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745757036.291957  859554 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745757036.295666  859554 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745757036.305713  859554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745757036.305731  859554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745757036.305732  859554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745757036.305733  859554 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-27 14:30:36.309117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import xgboost\n",
    "import warnings\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from scipy.special import expit\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9da781-745c-451d-b597-1aa9c0499ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df_main    = pd.read_excel(\"btUTgX.xlsx\")\n",
    "    complaints = pd.read_csv(\"newdataset2.csv\")\n",
    "\n",
    "    kws = [\n",
    "        \"Service Quality\",\n",
    "        \"Inconsistent Internet Speed\",\n",
    "        \"No Proactive Support\",\n",
    "        \"Overcharging\",\n",
    "        \"Not Communicated Extra Charges\"\n",
    "    ]\n",
    "\n",
    "    # compute raw sentiment per complaint\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    complaints['sentiment_score'] = (\n",
    "        complaints['complaint']\n",
    "        .fillna(\"\")\n",
    "        .apply(lambda txt: analyzer.polarity_scores(txt)['compound'])\n",
    "    )\n",
    "\n",
    "    # group\n",
    "    grouped            = complaints.groupby(\"customerID\")\n",
    "    #complaint_count    = grouped.size().rename(\"complaintAmount\")\n",
    "    #complaints_kw_agg  = grouped[kws].any()\n",
    "    sentiment_agg      = grouped['sentiment_score'].sum()\n",
    "\n",
    "    min_s, max_s = sentiment_agg.min(), sentiment_agg.max()\n",
    "    sentiment_scaled = -((sentiment_agg - max_s) / (max_s - min_s)).rename(\"complaintScore\")\n",
    "\n",
    "    # combine into summary\n",
    "    complaint_summary = sentiment_scaled\n",
    "\n",
    "    #complaint_summary[\"hasComplained\"] = complaint_summary[\"complaintAmount\"] > 0\n",
    "\n",
    "    # merge & fill\n",
    "    df_main = df_main.merge(complaint_summary, on=\"customerID\", how=\"left\")\n",
    "    #df_main[\"complaintAmount\"]  = df_main[\"complaintAmount\"].fillna(0).astype(int)\n",
    "    #df_main[\"hasComplained\"]    = df_main[\"hasComplained\"].fillna(False)\n",
    "    df_main[\"complaintScore\"]   = df_main[\"complaintScore\"].fillna(0.0)\n",
    "    #for col in kws:\n",
    "    #    df_main[col] = df_main[col].fillna(False)\n",
    "\n",
    "    return df_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f3cec5-34f2-42c1-81ab-101e72dad224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_and_scaler(load_xgb=True, load_lr=True, load_scaler=True):\n",
    "    xgb_model = None\n",
    "    lr_model = None\n",
    "    scaler = None\n",
    "\n",
    "    if load_xgb:\n",
    "        xgb_model = xgboost.Booster()\n",
    "        xgb_model.load_model('xgb_model.json')\n",
    "    \n",
    "    if load_lr:\n",
    "        lr_model = joblib.load('lr_model.pkl')\n",
    "    \n",
    "    if load_scaler:\n",
    "        scaler = joblib.load('scaler.pkl')\n",
    "    \n",
    "    return xgb_model, lr_model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e633f2e-681d-44bb-861e-c1ef3133df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(inp):\n",
    "    # Drop the customerID column\n",
    "    inp = inp.drop('customerID', axis=1)\n",
    "    \n",
    "    df = inp.copy()\n",
    "    df.loc[df['tenure'] == 0, 'TotalCharges'] = 0\n",
    "    \n",
    "    # Map service-related columns\n",
    "    mapping_phone = {\"No\": 0, \"Yes\": 1}\n",
    "    mapping_multi = {\"No\": 0, \"No phone service\": 0, \"Yes\": 1}\n",
    "    mapping_internet = {\"No\": 0, \"DSL\": 1, \"Fiber optic\": 1}\n",
    "    \n",
    "    for col, mapping in [(\"PhoneService\", mapping_phone),\n",
    "                         (\"MultipleLines\", mapping_multi),\n",
    "                         (\"InternetService\", mapping_internet)]:\n",
    "        df[col] = df[col].map(mapping)\n",
    "    \n",
    "    # Map additional service columns\n",
    "    service_cols = [\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \n",
    "                    \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\n",
    "    for col in service_cols:\n",
    "        df[col] = df[col].map({\"No\": 0, \"Yes\": 1, \"No internet service\": 0})\n",
    "    \n",
    "    # Create new feature based on service features\n",
    "    inp[\"featurePerCharged\"] = df[[\"PhoneService\", \"MultipleLines\", \"InternetService\", \n",
    "                                   \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \n",
    "                                   \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]].sum(axis=1) / df[\"MonthlyCharges\"]\n",
    "    \n",
    "    # Map additional categorical columns\n",
    "    inp[\"gender\"] = inp[\"gender\"].map({\"Female\": 0, \"Male\": 1}).astype(\"category\")\n",
    "    inp[\"SeniorCitizen\"] = inp[\"SeniorCitizen\"].astype(\"category\")\n",
    "    for col in [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\"]:\n",
    "        inp[col] = inp[col].map({\"No\": 0, \"Yes\": 1}).astype(\"category\")\n",
    "    \n",
    "    # Convert numeric columns and map churn\n",
    "    inp[[\"MonthlyCharges\", \"TotalCharges\"]] = df[[\"MonthlyCharges\", \"TotalCharges\"]].astype(\"float32\")\n",
    "    inp[\"Churn\"] = df[\"Churn\"].map({\"No\": 0, \"Yes\": 1})\n",
    "    \n",
    "    # Bin tenure into categories\n",
    "    bins = [0, 12, 24, 48, np.inf]\n",
    "    labels = [0, 1, 2, 3]\n",
    "    inp[\"tenure_binned\"] = pd.cut(inp[\"tenure\"], bins=bins, labels=labels)\n",
    "    inp.loc[inp[\"tenure\"] == 0, 'tenure_binned'] = 0\n",
    "    \n",
    "    # Create dummy variables for selected categorical columns\n",
    "    categorical_cols = ['Contract', 'PaymentMethod', 'tenure_binned', \"MultipleLines\", \n",
    "                        'InternetService', \"OnlineSecurity\", \"OnlineBackup\", \n",
    "                        'DeviceProtection', \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\n",
    "    inp = pd.get_dummies(inp, columns=categorical_cols, drop_first=False, dtype=\"category\")\n",
    "    \n",
    "    # Drop columns with unwanted keywords\n",
    "    to_drop = [col for col in inp.columns \n",
    "               if (\"No internet service\" in col or \"No phone service\" in col or \n",
    "                   \"InternetService_No\" in col or \"One year\" in col or \n",
    "                   \"Mailed\" in col or \"binned_0\" in col)]\n",
    "    inp.drop(to_drop, inplace=True, axis=1)\n",
    "    \n",
    "    # Prepare features for clustering\n",
    "    cluster_features = ['tenure_binned_1', 'tenure_binned_2', 'tenure_binned_3', \n",
    "                        'Contract_Month-to-month', 'Contract_Two year', \n",
    "                        'TechSupport_Yes', 'TechSupport_No', \n",
    "                        'OnlineSecurity_No', 'OnlineSecurity_Yes', \n",
    "                        \"InternetService_DSL\", 'InternetService_Fiber optic']\n",
    "    inp_cluster = inp[cluster_features]\n",
    "    \n",
    "    # Load pre-trained KMeans model and assign clusters\n",
    "    kmeans = joblib.load('kmeans.pkl')\n",
    "    clusters = kmeans.predict(inp_cluster)\n",
    "    inp[\"cluster\"] = clusters\n",
    "    inp[\"cluster\"] = inp[\"cluster\"].astype(\"category\")\n",
    "\n",
    "    for col in inp.drop(columns=[\"MonthlyCharges\", \"TotalCharges\", \"tenure\", \"featurePerCharged\", \"complaintScore\"], axis=1).columns:\n",
    "        inp[col] = inp[col].astype(int)\n",
    "    \n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289505d5-fd25-415f-b52f-793c9159a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def apply_logical_constraints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1) PhoneService → MultipleLines_No\n",
    "    if {\"PhoneService\", \"MultipleLines_No\"}.issubset(df.columns):\n",
    "        mask = df[\"PhoneService\"] == 0\n",
    "        df.loc[mask, \"MultipleLines_No\"] = 1\n",
    "\n",
    "    # 2) Contract mutual exclusion\n",
    "    c1, c2 = \"Contract_Month-to-month\", \"Contract_Two year\"\n",
    "    if {c1, c2}.issubset(df.columns):\n",
    "        both = (df[c1] == 1) & (df[c2] == 1)\n",
    "        idx = df.index[both]\n",
    "        if len(idx):\n",
    "            choices = np.random.choice([0, 1], size=len(idx))\n",
    "            df.loc[idx[choices == 0], c1] = 0\n",
    "            df.loc[idx[choices == 1], c2] = 0\n",
    "\n",
    "    # 3) PaymentMethod exclusivity\n",
    "    pm = [\n",
    "        \"PaymentMethod_Bank transfer (automatic)\",\n",
    "        \"PaymentMethod_Credit card (automatic)\",\n",
    "        \"PaymentMethod_Electronic check\"\n",
    "    ]\n",
    "    if set(pm).issubset(df.columns):\n",
    "        # Triple conflict\n",
    "        all3 = (df[pm] == 1).all(axis=1)\n",
    "        idx3 = df.index[all3]\n",
    "        for cid in idx3:\n",
    "            # choose one to keep randomly\n",
    "            keep = np.random.choice(pm)\n",
    "            for col in pm:\n",
    "                if col != keep:\n",
    "                    df.at[cid, col] = 0\n",
    "        # Pairwise conflicts\n",
    "        for a, b in [(pm[0], pm[1]), (pm[0], pm[2]), (pm[1], pm[2])]:\n",
    "            pair = (df[a] == 1) & (df[b] == 1) & ~all3\n",
    "            idx2 = df.index[pair]\n",
    "            if len(idx2):\n",
    "                choices = np.random.choice([0, 1], size=len(idx2))\n",
    "                df.loc[idx2[choices == 0], a] = 0\n",
    "                df.loc[idx2[choices == 1], b] = 0\n",
    "\n",
    "    # 4) InternetService exclusivity and dependent services\n",
    "    i1, i2 = \"InternetService_DSL\", \"InternetService_Fiber optic\"\n",
    "    deps = [\n",
    "        \"OnlineSecurity_No\", \"OnlineSecurity_Yes\",\n",
    "        \"OnlineBackup_No\",  \"OnlineBackup_Yes\",\n",
    "        \"DeviceProtection_No\", \"DeviceProtection_Yes\",\n",
    "        \"TechSupport_No\",  \"TechSupport_Yes\",\n",
    "        \"StreamingTV_No\",  \"StreamingTV_Yes\",\n",
    "        \"StreamingMovies_No\", \"StreamingMovies_Yes\"\n",
    "    ]\n",
    "    if {i1, i2}.issubset(df.columns):\n",
    "        # Mutual exclusivity\n",
    "        both_int = (df[i1] == 1) & (df[i2] == 1)\n",
    "        idx_both = df.index[both_int]\n",
    "        if len(idx_both):\n",
    "            choices = np.random.choice([i1, i2], size=len(idx_both))\n",
    "            for cid, choice in zip(idx_both, choices):\n",
    "                df.at[cid, choice] = 0\n",
    "        # No internet → zero all dependents\n",
    "        none_int = (df[i1] == 0) & (df[i2] == 0)\n",
    "        df.loc[none_int, deps] = 0\n",
    "        # Dependent yes/no exclusivity\n",
    "        for no_col, yes_col in zip(deps[0::2], deps[1::2]):\n",
    "            conflict = (df[no_col] == 1) & (df[yes_col] == 1)\n",
    "            idx_conf = df.index[conflict]\n",
    "            if len(idx_conf):\n",
    "                choices = np.random.choice([0, 1], size=len(idx_conf))\n",
    "                df.loc[idx_conf[choices == 0], no_col] = 0\n",
    "                df.loc[idx_conf[choices == 1], yes_col] = 0\n",
    "\n",
    "    # Ensure any bool columns are converted to 0/1 ints\n",
    "    bool_cols = df.select_dtypes(include='bool').columns\n",
    "    if len(bool_cols):\n",
    "        df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c363b0f1-945d-47d5-abd5-e3e23e57eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new = new_df.drop([\"Churn\", \"MonthlyCharges\", \"TotalCharges\", \"featurePerCharged\", \"complaintScore\"], axis=1).copy()\n",
    "    \n",
    "#model = joblib.load(\"monthly_charges_model.pkl\")\n",
    "\n",
    "#new_df[\"MonthlyCharges\"] = model.predict(X_new)\n",
    "\n",
    "#new_df[\"TotalCharges\"] = new_df[\"MonthlyCharges\"] * new_df[\"tenure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569c0a2f-91b7-4e7e-bad0-07e78b0f73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_new_observations(processed_df, N, kmeans):\n",
    "    processed_df = processed_df.loc[processed_df[\"tenure\"] <= 12].copy()\n",
    "    cols_to_sample = [col for col in processed_df.columns \n",
    "                      if col not in [\"Churn\", \"TotalCharges\", \"featurePerCharged\", \"tenure_binned_0\",  \"tenure_binned_1\", \"tenure_binned_2\", \"tenure_binned_3\"]]\n",
    "    \n",
    "    new_df = processed_df[cols_to_sample].sample(n=N, replace=True).reset_index(drop=True)\n",
    "    \n",
    "    new_df = apply_logical_constraints(new_df)\n",
    "    new_df[\"tenure\"] = 0\n",
    "    \n",
    "    bins = [0, 12, 24, 48, np.inf]\n",
    "    labels = [0, 1, 2, 3]\n",
    "    new_df[\"tenure_binned\"] = pd.cut(new_df[\"tenure\"].astype(int), bins=bins, labels=labels)\n",
    "    new_df.loc[new_df[\"tenure\"] == 0, 'tenure_binned'] = 0\n",
    "\n",
    "    new_df = pd.get_dummies(new_df, columns=['tenure_binned'], drop_first=False, dtype=\"category\")\n",
    "    new_df.drop('tenure_binned_0', axis=1, inplace=True)\n",
    "\n",
    "    new_df[\"TotalCharges\"] = 0.0\n",
    "\n",
    "    new_df = new_df.reindex(columns=processed_df.columns)\n",
    "    \n",
    "    yes_cols = [col for col in new_df.columns if col.endswith(\"_Yes\")]\n",
    "    if yes_cols:\n",
    "        new_df[\"featurePerCharged\"] = new_df[yes_cols].astype(int).sum(axis=1) / new_df[\"MonthlyCharges\"]\n",
    "    else:\n",
    "        new_df[\"featurePerCharged\"] = 0  \n",
    "\n",
    "    cluster_features = ['tenure_binned_1', 'tenure_binned_2', 'tenure_binned_3', \n",
    "                        'Contract_Month-to-month', 'Contract_Two year', \n",
    "                        'TechSupport_Yes', 'TechSupport_No', \n",
    "                        'OnlineSecurity_No', 'OnlineSecurity_Yes', \n",
    "                        \"InternetService_DSL\", 'InternetService_Fiber optic']\n",
    "    inp_cluster = new_df[cluster_features]\n",
    "    \n",
    "    clusters = kmeans.predict(inp_cluster)\n",
    "    new_df[\"cluster\"] = clusters\n",
    "    new_df[\"complaintScore\"] = 0.0\n",
    "\n",
    "    new_df[\"churn_pred\"] = 0\n",
    "    new_df[\"churn_prob\"] = 0.0\n",
    "    new_df[\"retent\"]     = 0\n",
    "    new_df[\"Churn\"]      = -1\n",
    "\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20075a29-f125-4885-a644-da0f05c7e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upd(cus_base: pd.DataFrame, processed_df: pd.DataFrame, kmeans) -> pd.DataFrame:\n",
    "    cus = cus_base.copy()\n",
    "\n",
    "    # 1) Cast tenure bins to bool and save old state\n",
    "    for col in [\"tenure_binned_1\", \"tenure_binned_2\", \"tenure_binned_3\"]:\n",
    "        cus[col] = cus[col].astype(bool)\n",
    "    old1 = cus[\"tenure_binned_1\"]\n",
    "    old2 = cus[\"tenure_binned_2\"]\n",
    "    old3 = cus[\"tenure_binned_3\"]\n",
    "\n",
    "    # 2) Bump tenure & recompute bins\n",
    "    cus[\"tenure\"] += 1\n",
    "    cus[\"tenure_binned_1\"] = (cus[\"tenure\"] > 12) & (cus[\"tenure\"] <= 24)\n",
    "    cus[\"tenure_binned_2\"] = (cus[\"tenure\"] > 24) & (cus[\"tenure\"] <= 48)\n",
    "    cus[\"tenure_binned_3\"] =  cus[\"tenure\"] > 48\n",
    "\n",
    "    # 3) Who just entered each bin?\n",
    "    entrants = {\n",
    "        1: (~old1) & cus[\"tenure_binned_1\"],\n",
    "        2: (~old2) & cus[\"tenure_binned_2\"],\n",
    "        3: (~old3) & cus[\"tenure_binned_3\"],\n",
    "    }\n",
    "\n",
    "    # 4) Derive contract_type in processed_df\n",
    "    def derive_contract(r):\n",
    "        if r[\"Contract_Month-to-month\"]:\n",
    "            return \"Month-to-month\"\n",
    "        elif r[\"Contract_Two year\"]:\n",
    "            return \"Two year\"\n",
    "        else:\n",
    "            return \"One year\"\n",
    "            \n",
    "    proc = processed_df.copy()\n",
    "    proc[\"contract_type\"] = proc.apply(derive_contract, axis=1)\n",
    "\n",
    "    # 5) Feature columns for all KNNs\n",
    "    drop_cols = {\"customerID\", \"Churn\", \"churn_prob\", \"churn_pred\", \"contract_type\"}\n",
    "    feat_cols = [c for c in proc.columns if c not in drop_cols]\n",
    "\n",
    "    # 6) Sample contracts via 30‑NN per bin\n",
    "    for b, mask in entrants.items():\n",
    "        idxs = cus.index[mask]\n",
    "        if not len(idxs):\n",
    "            continue\n",
    "\n",
    "        # subset processed_df to bin b\n",
    "        if b == 1:\n",
    "            proc_bin = proc[(proc[\"tenure\"] > 12) & (proc[\"tenure\"] <= 24)]\n",
    "        elif b == 2:\n",
    "            proc_bin = proc[(proc[\"tenure\"] > 24) & (proc[\"tenure\"] <= 48)]\n",
    "        else:\n",
    "            proc_bin = proc[proc[\"tenure\"] > 48]\n",
    "        if proc_bin.empty:\n",
    "            continue\n",
    "\n",
    "        Xb = proc_bin[feat_cols].values\n",
    "        nn_loc = NearestNeighbors(n_neighbors=min(30, len(Xb))).fit(Xb)\n",
    "        Xq = cus.loc[idxs, feat_cols].values\n",
    "        _, nbrs = nn_loc.kneighbors(Xq)\n",
    "\n",
    "        for cust_i, nbr_idx in zip(idxs, nbrs):\n",
    "            neigh = proc_bin.iloc[nbr_idx][\"contract_type\"]\n",
    "            probs = neigh.value_counts(normalize=True).reindex(\n",
    "                [\"Month-to-month\", \"Two year\", \"One year\"], fill_value=0.0\n",
    "            )\n",
    "            choice = np.random.choice(probs.index, p=probs.values)\n",
    "            cus.at[cust_i, \"Contract_Month-to-month\"] = int(choice == \"Month-to-month\")\n",
    "            cus.at[cust_i, \"Contract_Two year\"]      = int(choice == \"Two year\")\n",
    "            # One year → both False\n",
    "\n",
    "    # 7) Update complaintScore via 30‑NN, using DataFrames for clarity\n",
    "    features_df = processed_df[feat_cols]  # DataFrame with column names\n",
    "    nn_comp = NearestNeighbors(n_neighbors=min(100, features_df.shape[0])).fit(features_df)\n",
    "\n",
    "    for i in cus.index:\n",
    "        old_score = cus.at[i, \"complaintScore\"]\n",
    "        xq_df = cus.loc[[i], feat_cols]     # DataFrame (1 × n_features)\n",
    "        dists, inds = nn_comp.kneighbors(xq_df)\n",
    "\n",
    "        neigh_df = processed_df.iloc[inds[0]]\n",
    "        neigh_scores = neigh_df[\"complaintScore\"]\n",
    "\n",
    "        mask_nz = neigh_scores > 0\n",
    "        if mask_nz.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        comp_coeff = 0.12\n",
    "        p_complain = mask_nz.sum() / neigh_scores.shape[0] * comp_coeff\n",
    "        d_nz       = dists[0][mask_nz]\n",
    "        weights    = (1.0 / d_nz)\n",
    "        weights   /= weights.sum()\n",
    "        new_score  = np.dot(neigh_scores[mask_nz], weights)\n",
    "\n",
    "        if old_score == 0.0:\n",
    "            if np.random.rand() < p_complain:\n",
    "                cus.at[i, \"complaintScore\"] = new_score\n",
    "        else:\n",
    "            if new_score > old_score:\n",
    "                cus.at[i, \"complaintScore\"] = new_score\n",
    "\n",
    "    # 8) Update TotalCharges\n",
    "    cus[\"TotalCharges\"] += cus[\"MonthlyCharges\"]\n",
    "\n",
    "    # 9) Re‑cluster — pass a DataFrame to preserve feature names\n",
    "    cluster_features = [\n",
    "        'tenure_binned_1','tenure_binned_2','tenure_binned_3',\n",
    "        'Contract_Month-to-month','Contract_Two year',\n",
    "        'TechSupport_Yes','TechSupport_No',\n",
    "        'OnlineSecurity_No','OnlineSecurity_Yes',\n",
    "        'InternetService_DSL','InternetService_Fiber optic'\n",
    "    ]\n",
    "    \n",
    "    cus[\"cluster\"] = kmeans.predict(cus[cluster_features])\n",
    "\n",
    "    return cus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c800c57-d3d9-4511-a54d-edd478581f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_churn_probability(obs_df, processed_df, model, nn):\n",
    "    def tenure_churn_factor(tenure):\n",
    "        base = 0.05   # starting probability at tenure=0\n",
    "        peak = 0.15   # peak reached at tenure=5\n",
    "        mid  = 0.05  # value at tenure=12 before sharp drop\n",
    "        decay_rate = 0.45  # controls exponential decay after tenure=12\n",
    "        \n",
    "        if tenure <= 5:\n",
    "            # Increasing probability from base to peak\n",
    "            return base + (peak - base) * (tenure / 5)\n",
    "        elif tenure <= 12:\n",
    "            # Slight decrease from peak to mid between tenure 5 and 12\n",
    "            return peak - (peak - mid) * ((tenure - 5) / 7)\n",
    "        else:\n",
    "            # Sharper drop-off after tenure 12\n",
    "            return mid * np.exp(-decay_rate * (tenure - 12))\n",
    "    \n",
    "    # ---- Calculate churn ratio using nearest neighbors ----\n",
    "    # Select predictor columns (excluding churn-related outputs)\n",
    "    predictor_cols = [col for col in processed_df.columns if col not in [\"Churn\", \"churn_prob\", \"churn_pred\"]]\n",
    "    obs_features = obs_df[predictor_cols]\n",
    "    distances, indices = nn.kneighbors(obs_features)  # shape: (n_obs, n_neighbors)\n",
    "    \n",
    "    # For each observation, compute the average churn rate of its 10 nearest neighbors\n",
    "    churn_ratios = np.array([processed_df.iloc[idx][\"Churn\"].astype(bool).mean() for idx in indices])\n",
    "    \n",
    "    # ---- Prepare features for the logistic regression model ----\n",
    "    cols_drop = [\"Churn\", \"churn_pred\", \"churn_prob\", 'gender', 'Partner', 'Dependents']\n",
    "    obs_x = obs_df.drop(columns=cols_drop).copy()\n",
    "\n",
    "    pred_classes = model.predict(obs_x)\n",
    "    obs_df[\"churn_pred\"] = pred_classes\n",
    "    # Map prediction confidence: these values (0.85 and 0.62) can be refined per your model’s performance.\n",
    "    mapped_confidences = np.where(pred_classes == 0, 1 - 0.88, 0.61)\n",
    "    \n",
    "    # ---- Incorporate the tenure factor ----\n",
    "    tenure_values = obs_df[\"tenure\"].values.astype(np.float32)\n",
    "    # Compute the tenure factor for each observation\n",
    "    tenure_factors = np.array([tenure_churn_factor(t) for t in tenure_values])\n",
    "    \n",
    "    # Final churn probability per tick is the product of:\n",
    "    # 1. The churn ratio from similar neighbors\n",
    "    # 2. The mapped model confidence\n",
    "    # 3. The tenure-based adjustment\n",
    "    final_probs = churn_ratios * mapped_confidences * tenure_factors\n",
    "    obs_df[\"churn_prob\"] = final_probs\n",
    "\n",
    "    return final_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7dc040-d11b-4d95-a635-9a7a1dba79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "processed_df = process_data(df)\n",
    "\n",
    "_, lr_model, _ = load_models_and_scaler()\n",
    "\n",
    "nn_churn = joblib.load(\"nn_forProb.pkl\")\n",
    "kmeans = joblib.load('kmeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f2b6e79-2e2c-4a8a-a73d-c4b65e3f56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0) Fit Kaplan–Meier once, at module load\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "kmf = KaplanMeierFitter()\n",
    "T = processed_df[\"tenure\"].values\n",
    "E = processed_df[\"Churn\"].astype(bool).values\n",
    "kmf.fit(T, event_observed=E)\n",
    "\n",
    "sf = kmf.survival_function_\n",
    "times  = sf.index.values.astype(np.float32)\n",
    "surv   = sf[\"KM_estimate\"].values.astype(np.float32)\n",
    "\n",
    "# baseline hazard h0[t] ≈ [S(t) - S(t+1)]/S(t)\n",
    "hazard = (surv[:-1] - surv[1:]) / surv[:-1]\n",
    "hazard = np.concatenate([hazard, [0.0]])  # pad last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "014db851-7d0c-4c7f-8829-d8b47f6adf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_remaining_hybrid(obs_df: pd.DataFrame,\n",
    "                          processed_df: pd.DataFrame,\n",
    "                          lr_model,\n",
    "                          nn_churn,\n",
    "                          kmeans,\n",
    "                          max_horizon: int = 60) -> np.ndarray:\n",
    "    df = obs_df.copy()\n",
    "    N = df.shape[0]\n",
    "    \n",
    "    # We'll store survivors[k] = Pr(alive after k steps)\n",
    "    survivors = np.ones(N, dtype=np.float32)\n",
    "    rem       = np.zeros(N, dtype=np.float32)\n",
    "    \n",
    "    for k in range(1, max_horizon+1):\n",
    "        # 1) compute churn_prob at this step (hybrid)\n",
    "        p = calc_churn_probability(df, processed_df, lr_model, nn_churn)\n",
    "        # 2) update survival\n",
    "        survivors *= (1.0 - p)\n",
    "        # 3) accumulate\n",
    "        rem += survivors\n",
    "        # 4) update accordingly\n",
    "        df = upd(processed_df=processed_df, cus_base=df, kmeans=kmeans)\n",
    "        \n",
    "    return rem  # shape (N,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674e0eb7-c96f-4458-a2f5-6f14d0c35409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h0_of_tenure(t_array):\n",
    "    idx = np.searchsorted(times, t_array, side=\"right\") - 1\n",
    "    idx = np.clip(idx, 0, len(hazard)-1)\n",
    "    return hazard[idx]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Hybrid KM × personal score churn‐prob\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def calc_churn_probability_hybrid(obs_df, processed_df, model, nn):\n",
    "    # (a) personal churn ratio via k-NN\n",
    "    predictor_cols = [c for c in processed_df.columns\n",
    "                      if c not in [\"Churn\",\"churn_prob\",\"churn_pred\"]]\n",
    "    dists, inds = nn.kneighbors(obs_df[predictor_cols])\n",
    "    churn_ratios = np.array([processed_df.iloc[i][\"Churn\"].mean() for i in inds])\n",
    "\n",
    "    # (b) logistic‐model confidence\n",
    "    drop = [\"Churn\",\"churn_pred\",\"churn_prob\",\"gender\",\"Partner\",\"Dependents\"]\n",
    "    pred_cls = model.predict(obs_df.drop(columns=drop))\n",
    "    obs_df[\"churn_pred\"] = pred_cls\n",
    "    conf = np.where(pred_cls==0, 1-0.88, 0.61)\n",
    "\n",
    "    personal = churn_ratios * conf\n",
    "\n",
    "    # (c) baseline hazard\n",
    "    tenures = obs_df[\"tenure\"].values.astype(np.float32)\n",
    "    h0      = h0_of_tenure(tenures)\n",
    "\n",
    "    # (d) normalize personal so avg(personal) at each tenure = 1\n",
    "    df = obs_df.copy()\n",
    "    df[\"personal\"] = personal\n",
    "    mean_per_t = df.groupby(\"tenure\")[\"personal\"].transform(\"mean\")\n",
    "    hybrid = h0 * (personal / (mean_per_t + 1e-8))\n",
    "\n",
    "    obs_df[\"churn_prob\"] = hybrid.astype(np.float32)\n",
    "    return hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cb66c9c-f521-42ee-909b-c977552bd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [\n",
    "    'gender','SeniorCitizen','Partner','Dependents','tenure',\n",
    "    'PhoneService','PaperlessBilling','MonthlyCharges','TotalCharges',\n",
    "    'complaintScore','featurePerCharged',\n",
    "    'Contract_Month-to-month','Contract_Two year',\n",
    "    'PaymentMethod_Bank transfer (automatic)',\n",
    "    'PaymentMethod_Credit card (automatic)',\n",
    "    'PaymentMethod_Electronic check',\n",
    "    'tenure_binned_1','tenure_binned_2','tenure_binned_3',\n",
    "    'MultipleLines_No','MultipleLines_Yes',\n",
    "    'InternetService_DSL','InternetService_Fiber optic',\n",
    "    'OnlineSecurity_No','OnlineSecurity_Yes',\n",
    "    'OnlineBackup_No','OnlineBackup_Yes',\n",
    "    'DeviceProtection_No','DeviceProtection_Yes',\n",
    "    'TechSupport_No','TechSupport_Yes',\n",
    "    'StreamingTV_No','StreamingTV_Yes',\n",
    "    'StreamingMovies_No','StreamingMovies_Yes',\n",
    "    'cluster'\n",
    "]\n",
    "P = len(feat_cols)\n",
    "\n",
    "# Number of heads:\n",
    "# 1 discount + 1 PhoneService + 3 internet + 6 service‐pairs = 11\n",
    "num_heads = 11\n",
    "\n",
    "# Names of the six yes/no service pairs\n",
    "pair_names = [\n",
    "    'OnlineSecurity','OnlineBackup','DeviceProtection',\n",
    "    'TechSupport','StreamingTV','StreamingMovies'\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# small softmax implementation\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def softmax(x, axis=1):\n",
    "    e = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Unpack θ into per‐head weights & biases\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def unpack_theta(theta):\n",
    "    offset = 0\n",
    "    heads = {}\n",
    "    # discount\n",
    "    heads['disc_w'] = theta[offset:offset+P]; heads['disc_b'] = theta[offset+P]\n",
    "    offset += P+1\n",
    "    # PhoneService\n",
    "    heads['ps_w']   = theta[offset:offset+P]; heads['ps_b']   = theta[offset+P]\n",
    "    offset += P+1\n",
    "    # internet: 3 heads\n",
    "    heads['int_w']  = theta[offset:offset+3*P].reshape(3, P)\n",
    "    offset += 3*P\n",
    "    heads['int_b']  = theta[offset:offset+3]\n",
    "    offset += 3\n",
    "    # service pairs: 6 heads\n",
    "    heads['pair_w'] = theta[offset:offset+6*P].reshape(6, P)\n",
    "    offset += 6*P\n",
    "    heads['pair_b'] = theta[offset:offset+6]\n",
    "    offset += 6\n",
    "    return heads\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) apply_retention using the structured heads\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def apply_retention(offered_df: pd.DataFrame, theta) -> pd.DataFrame:\n",
    "    df2 = offered_df.copy()\n",
    "    heads = unpack_theta(theta)\n",
    "    X = df2[feat_cols].to_numpy(dtype=np.float32)  # (M, P)\n",
    "\n",
    "    # -- discount head --\n",
    "    disc_lin = X.dot(heads['disc_w']) + heads['disc_b']  # (M,)\n",
    "    discounts = expit(disc_lin)                         # (M,)\n",
    "    df2['MonthlyCharges'] *= (1.0 - discounts)\n",
    "\n",
    "    # -- PhoneService head (binary sigmoid) --\n",
    "    ps_lin = X.dot(heads['ps_w']) + heads['ps_b']\n",
    "    p_ps = expit(ps_lin)\n",
    "    df2['PhoneService'] = (p_ps > 0.5).astype(int)\n",
    "\n",
    "    # -- InternetService head (3‐way softmax: 0=No,1=DSL,2=Fiber) --\n",
    "    int_lin = X.dot(heads['int_w'].T) + heads['int_b'][None, :]  # (M,3)\n",
    "    int_prob = softmax(int_lin, axis=1)                         # (M,3)\n",
    "    choice = np.argmax(int_prob, axis=1)                        # (M,)\n",
    "    df2['InternetService_DSL']         = (choice == 1).astype(int)\n",
    "    df2['InternetService_Fiber optic'] = (choice == 2).astype(int)\n",
    "\n",
    "    # -- Six yes/no pairs (sigmoid→threshold) --\n",
    "    for i, name in enumerate(pair_names):\n",
    "        lin = X.dot(heads['pair_w'][i]) + heads['pair_b'][i]   # (M,)\n",
    "        p  = expit(lin)\n",
    "        yes = (p > 0.5).astype(int)\n",
    "        no  = 1 - yes\n",
    "        df2[f'{name}_Yes'] = yes\n",
    "        df2[f'{name}_No']  = no\n",
    "\n",
    "\n",
    "    yes_cols = [col for col in df2.columns if col.endswith(\"_Yes\")]\n",
    "    if yes_cols:\n",
    "        df2[\"featurePerCharged\"] = df2[yes_cols].astype(int).sum(axis=1) / (df2[\"MonthlyCharges\"] + 1e-3)\n",
    "    else:\n",
    "        df2[\"featurePerCharged\"] = 0  \n",
    "\n",
    "\n",
    "    return df2\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) reward_func\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "toggle_costs = {\n",
    "    'PhoneService':               2.0,\n",
    "    'InternetService_DSL':        5.0,\n",
    "    'InternetService_Fiber optic':7.0,\n",
    "    'OnlineSecurity_Yes':         1.0,\n",
    "    'OnlineBackup_Yes':           1.5,\n",
    "    'DeviceProtection_Yes':       2.0,\n",
    "    'TechSupport_Yes':            3.0,\n",
    "    'StreamingTV_Yes':            1.0,\n",
    "    'StreamingMovies_Yes':        1.0,\n",
    "}\n",
    "\n",
    "def reward_func(pred_df: pd.DataFrame,\n",
    "                offered_df: pd.DataFrame,\n",
    "                processed_df: pd.DataFrame,\n",
    "                lr_model,\n",
    "                nn_churn,\n",
    "                kmeans,\n",
    "                toggle_costs: dict,\n",
    "                max_horizon: int = 60) -> float:\n",
    " \n",
    "    # 1) copy and recompute churn_prob on both sets\n",
    "    pred_df = pred_df.copy()\n",
    "    off_df  = offered_df.copy()\n",
    "\n",
    "    rem_pred = cond_remaining_hybrid(pred_df,\n",
    "                                processed_df,\n",
    "                                lr_model,\n",
    "                                nn_churn,\n",
    "                                kmeans,\n",
    "                                max_horizon)\n",
    "    \n",
    "    rem_offered = cond_remaining_hybrid(off_df,\n",
    "                                processed_df,\n",
    "                                lr_model,\n",
    "                                nn_churn,\n",
    "                                kmeans,\n",
    "                                max_horizon)\n",
    "\n",
    "    # 3) Expected future revenue BEFORE & AFTER\n",
    "    rev_before = ((1 - pred_df[\"churn_prob\"]) * pred_df[\"MonthlyCharges\"] * rem_pred).sum()\n",
    "    rev_after  = ((1 - off_df[\"churn_prob\"])  * off_df[\"MonthlyCharges\"]  * rem_offered).sum()\n",
    "    revenue_saved = rev_after - rev_before\n",
    "\n",
    "    # 4) Cost of any toggles turned ON\n",
    "    toggle_cols = [\n",
    "        'PhoneService',\n",
    "        'InternetService_DSL','InternetService_Fiber optic',\n",
    "        'OnlineSecurity_Yes','OnlineBackup_Yes',\n",
    "        'DeviceProtection_Yes','TechSupport_Yes',\n",
    "        'StreamingTV_Yes','StreamingMovies_Yes'\n",
    "    ]\n",
    "    # build a pd.Series of costs indexed by col-name\n",
    "    cost_s = pd.Series(toggle_costs)\n",
    "\n",
    "    # BEFORE: how much did we “plan” to spend per customer?\n",
    "    pred_on = pred_df[toggle_cols] == 1\n",
    "    pred_cost_per_cust = pred_on.mul(cost_s, axis=1).sum(axis=1)  # (n_pred,)\n",
    "\n",
    "    # AFTER:\n",
    "    off_on  = off_df[toggle_cols] == 1\n",
    "    off_cost_per_cust  = off_on .mul(cost_s, axis=1).sum(axis=1)  # (n_pred,)\n",
    "\n",
    "    # 4) Multiply by remaining‐months to get *future* cost\n",
    "    future_cost_before = (pred_cost_per_cust * rem_pred).sum()\n",
    "    future_cost_after  = (off_cost_per_cust  * rem_offered ).sum()\n",
    "\n",
    "    # 5) Δ future toggle cost\n",
    "    cost_of_toggles_deviation = future_cost_after - future_cost_before\n",
    "\n",
    "    # 6) Net reward = revenue_saved – extra toggle‐costs\n",
    "    net = revenue_saved - cost_of_toggles_deviation\n",
    "\n",
    "    return float(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b73bc4-5224-4621-8249-6a289fc09d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_tick_real(cus_base, processed_df,\n",
    "                  lr_model, nn_churn, kmeans,\n",
    "                  theta, spawn_n=10):\n",
    "\n",
    "    calc_churn_probability(cus_base, processed_df, lr_model, nn_churn)\n",
    "    # identify who to offer\n",
    "    mask = (cus_base['churn_pred']==1) & (cus_base['retent']==0)\n",
    "    predicted = cus_base.loc[mask].copy()\n",
    "    \n",
    "    r = 0\n",
    "    \n",
    "    if predicted.shape[0] > 0:\n",
    "        # apply retention\n",
    "        offered = apply_retention(predicted, theta)\n",
    "    \n",
    "        # recalc churn preds & probs for those offered\n",
    "        calc_churn_probability(offered, processed_df, lr_model, nn_churn)\n",
    "    \n",
    "        # reward\n",
    "        r = reward_func(offered_df=offered, \n",
    "                        kmeans=kmeans,\n",
    "                        lr_model=lr_model,\n",
    "                        max_horizon=60,\n",
    "                        nn_churn=nn_churn,\n",
    "                        pred_df=predicted,processed_df=processed_df, toggle_costs=toggle_costs)\n",
    "    \n",
    "        ## right before your assignment in simulate_tick:\n",
    "        for col in offered.columns:\n",
    "            # grab the target dtype from cus_base\n",
    "            target_dtype = cus_base[col].dtype\n",
    "            offered[col] = offered[col].astype(target_dtype)\n",
    "        \n",
    "        cus_base.loc[mask, offered.columns] = offered\n",
    "    \n",
    "\n",
    "    # revenue & churn removal\n",
    "    tick_rev  = cus_base['MonthlyCharges'].sum()\n",
    "    probs     = cus_base['churn_prob']\n",
    "    randoms   = np.random.rand(cus_base.shape[0])\n",
    "    churn_mask= randoms < probs\n",
    "    cus_base  = cus_base.loc[~churn_mask].reset_index(drop=True).copy()\n",
    "\n",
    "    # update survivors & spawn\n",
    "    cus_base = upd(cus_base, processed_df, kmeans)\n",
    "    new_obs  = spawn_new_observations(processed_df, spawn_n, kmeans)\n",
    "    new_obs['churn_pred'] = -1\n",
    "    new_obs['retent']     = 0\n",
    "    new_obs['churn_prob'] = 0.0\n",
    "    new_obs['Churn']      = -1\n",
    "    cus_base = pd.concat([cus_base, new_obs], ignore_index=True)\n",
    "\n",
    "    return tick_rev, cus_base, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "213c6d27-6402-4ec1-b845-19f6e51bfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_tick(cus_base, processed_df,\n",
    "                  lr_model, nn_churn, kmeans,\n",
    "                  theta):\n",
    "\n",
    "    calc_churn_probability(cus_base, processed_df, lr_model, nn_churn)\n",
    "    mask = (cus_base['churn_pred']==1)\n",
    "    predicted = cus_base.loc[mask].copy()\n",
    "    \n",
    "    r = 0\n",
    "    \n",
    "    if predicted.shape[0] > 0:\n",
    "        # apply retention\n",
    "        offered = apply_retention(predicted, theta)\n",
    "    \n",
    "        # recalc churn preds & probs for those offered\n",
    "        calc_churn_probability(offered, processed_df, lr_model, nn_churn)\n",
    "    \n",
    "        # reward\n",
    "        r = reward_func(offered_df=offered, \n",
    "                        kmeans=kmeans,\n",
    "                        lr_model=lr_model,\n",
    "                        max_horizon=12,\n",
    "                        nn_churn=nn_churn,\n",
    "                        pred_df=predicted,processed_df=processed_df, toggle_costs=toggle_costs)\n",
    "    \n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1756e73-7e7c-468a-9aa2-75142df6d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_random(processed_df, N, kmeans):\n",
    "    \n",
    "    new_df = processed_df[processed_df.columns].sample(n=N, replace=True).reset_index(drop=True)\n",
    "\n",
    "    new_df[\"churn_pred\"] = 0\n",
    "    new_df[\"churn_prob\"] = 0.0\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd06fd18-5a99-47f4-8c06-5b84c93d49f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Searching for the next optimal point.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_859554/1992814427.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# ───────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# 4) Launch Bayesian optimization\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# ───────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m res_bo = gp_minimize(\n\u001b[32m     61\u001b[39m     func            = bo_objective,\n\u001b[32m     62\u001b[39m     dimensions      = space,\n\u001b[32m     63\u001b[39m     acq_func        = \u001b[33m\"EI\"\u001b[39m,             \u001b[38;5;66;03m# Expected Improvement\u001b[39;00m\n",
      "\u001b[32m~/hacklab/env/lib/python3.11/site-packages/skopt/optimizer/gp.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[39m\n\u001b[32m    277\u001b[39m             random_state=rng.randint(\u001b[32m0\u001b[39m, np.iinfo(np.int32).max),\n\u001b[32m    278\u001b[39m             noise=noise,\n\u001b[32m    279\u001b[39m         )\n\u001b[32m    280\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     return base_minimize(\n\u001b[32m    282\u001b[39m         func,\n\u001b[32m    283\u001b[39m         space,\n\u001b[32m    284\u001b[39m         base_estimator=base_estimator,\n",
      "\u001b[32m~/hacklab/env/lib/python3.11/site-packages/skopt/optimizer/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[39m\n\u001b[32m    328\u001b[39m \n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;28;01min\u001b[39;00m range(n_calls):\n\u001b[32m    331\u001b[39m         next_x = optimizer.ask()\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m         next_y = func(next_x)\n\u001b[32m    333\u001b[39m         result = optimizer.tell(next_x, next_y)\n\u001b[32m    334\u001b[39m         result.specs = specs\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, result):\n",
      "\u001b[32m/tmp/ipykernel_859554/1992814427.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m bo_objective(x):\n\u001b[32m     40\u001b[39m     theta = np.array(x, dtype=np.float32)\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# average over a few episodes to smooth noise\u001b[39;00m\n\u001b[32m     42\u001b[39m     n_eps = \u001b[32m5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     rewards = [\n\u001b[32m     44\u001b[39m         run_episode(\n\u001b[32m     45\u001b[39m             theta,\n\u001b[32m     46\u001b[39m             processed_df,\n",
      "\u001b[32m/tmp/ipykernel_859554/1992814427.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m bo_objective(x):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     theta = np.array(x, dtype=np.float32)\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# average over a few episodes to smooth noise\u001b[39;00m\n\u001b[32m     46\u001b[39m     n_eps = \u001b[32m5\u001b[39m\n\u001b[32m     47\u001b[39m     rewards = [\n",
      "\u001b[32m/tmp/ipykernel_859554/1992814427.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(theta, processed_df, lr_model, nn_churn, kmeans, max_steps)\u001b[39m\n\u001b[32m     13\u001b[39m     total_reward = \u001b[32m0.0\u001b[39m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;28;01min\u001b[39;00m range(max_steps):\n\u001b[32m     16\u001b[39m         cus = spawn_random(processed_df, \u001b[32m100\u001b[39m, kmeans)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         r = simulate_tick(\n\u001b[32m     18\u001b[39m             cus,\n\u001b[32m     19\u001b[39m             processed_df,\n\u001b[32m     20\u001b[39m             lr_model,\n",
      "\u001b[32m/tmp/ipykernel_859554/2745043839.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cus_base, processed_df, lr_model, nn_churn, kmeans, theta)\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[38;5;66;03m# recalc churn preds & probs for those offered\u001b[39;00m\n\u001b[32m     16\u001b[39m         calc_churn_probability(offered, processed_df, lr_model, nn_churn)\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m         \u001b[38;5;66;03m# reward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         r = reward_func(offered_df=offered, \n\u001b[32m     20\u001b[39m                         kmeans=kmeans,\n\u001b[32m     21\u001b[39m                         lr_model=lr_model,\n\u001b[32m     22\u001b[39m                         max_horizon=\u001b[32m12\u001b[39m,\n",
      "\u001b[32m/tmp/ipykernel_859554/3585877820.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(pred_df, offered_df, processed_df, lr_model, nn_churn, kmeans, toggle_costs, max_horizon)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# 1) copy and recompute churn_prob on both sets\u001b[39;00m\n\u001b[32m    136\u001b[39m     pred_df = pred_df.copy()\n\u001b[32m    137\u001b[39m     off_df  = offered_df.copy()\n\u001b[32m    138\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     rem_pred = cond_remaining_hybrid(pred_df,\n\u001b[32m    140\u001b[39m                                 processed_df,\n\u001b[32m    141\u001b[39m                                 lr_model,\n\u001b[32m    142\u001b[39m                                 nn_churn,\n",
      "\u001b[32m/tmp/ipykernel_859554/1659406006.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obs_df, processed_df, lr_model, nn_churn, kmeans, max_horizon)\u001b[39m\n\u001b[32m     18\u001b[39m         survivors *= (\u001b[32m1.0\u001b[39m - p)\n\u001b[32m     19\u001b[39m         \u001b[38;5;66;03m# 3) accumulate\u001b[39;00m\n\u001b[32m     20\u001b[39m         rem += survivors\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# 4) update accordingly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         df = upd(processed_df=processed_df, cus_base=df, kmeans=kmeans)\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rem  \u001b[38;5;66;03m# shape (N,)\u001b[39;00m\n",
      "\u001b[32m/tmp/ipykernel_859554/556418892.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cus_base, processed_df, kmeans)\u001b[39m\n\u001b[32m     74\u001b[39m \n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m cus.index:\n\u001b[32m     76\u001b[39m         old_score = cus.at[i, \u001b[33m\"complaintScore\"\u001b[39m]\n\u001b[32m     77\u001b[39m         xq_df = cus.loc[[i], feat_cols]     \u001b[38;5;66;03m# DataFrame (1 × n_features)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         dists, inds = nn_comp.kneighbors(xq_df)\n\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m         neigh_df = processed_df.iloc[inds[\u001b[32m0\u001b[39m]]\n\u001b[32m     81\u001b[39m         neigh_scores = neigh_df[\u001b[33m\"complaintScore\"\u001b[39m]\n",
      "\u001b[32m~/hacklab/env/lib/python3.11/site-packages/sklearn/neighbors/_base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    834\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    835\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self.metric == \u001b[33m\"precomputed\"\u001b[39m:\n\u001b[32m    836\u001b[39m                 X = _check_precomputed(X)\n\u001b[32m    837\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m                 X = validate_data(\n\u001b[32m    839\u001b[39m                     self,\n\u001b[32m    840\u001b[39m                     X,\n\u001b[32m    841\u001b[39m                     ensure_all_finite=ensure_all_finite,\n",
      "\u001b[32m~/hacklab/env/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2940\u001b[39m             out = y\n\u001b[32m   2941\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2942\u001b[39m             out = X, y\n\u001b[32m   2943\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2945\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2947\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/hacklab/env/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1052\u001b[39m                         )\n\u001b[32m   1053\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1055\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m                 raise ValueError(\n\u001b[32m   1058\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m                 ) from complex_warning\n",
      "\u001b[32m~/hacklab/env/lib/python3.11/site-packages/sklearn/utils/_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    835\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    836\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    837\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    840\u001b[39m \n\u001b[32m    841\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~/hacklab/env/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m6284\u001b[39m     @final\n\u001b[32m   6285\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __getattr__(self, name: str):\n\u001b[32m   6286\u001b[39m         \"\"\"\n\u001b[32m   6287\u001b[39m         After regular attribute access, \u001b[38;5;28;01mtry\u001b[39;00m looking up the name\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.callbacks import VerboseCallback\n",
    "\n",
    "# 1) Episode runner\n",
    "def run_episode(theta,\n",
    "                processed_df,\n",
    "                lr_model,\n",
    "                nn_churn,\n",
    "                kmeans,\n",
    "                max_steps=12\n",
    "    ):\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        cus = spawn_random(processed_df, 100, kmeans)\n",
    "        r = simulate_tick(\n",
    "            cus,\n",
    "            processed_df,\n",
    "            lr_model,\n",
    "            nn_churn,\n",
    "            kmeans,\n",
    "            theta,\n",
    "        )\n",
    "        total_reward += r\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "P = len(feat_cols)       # e.g. 36\n",
    "num_heads = 11           # discount + phone + 3 internet + 6 service‐pairs\n",
    "theta_length = num_heads * (P + 1)\n",
    "\n",
    "space = [Real(-5.0, 5.0, name=f\"θ{i}\") for i in range(theta_length)]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Wrap your run_episode into a minimization target\n",
    "#    (we minimize –avg_reward to maximize avg_reward)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def bo_objective(x):\n",
    "    theta = np.array(x, dtype=np.float32)\n",
    "    # average over a few episodes to smooth noise\n",
    "    n_eps = 5\n",
    "    rewards = [\n",
    "        run_episode(\n",
    "            theta,\n",
    "            processed_df,\n",
    "            lr_model,\n",
    "            nn_churn,\n",
    "            kmeans,\n",
    "            max_steps=24,\n",
    "        )\n",
    "        for _ in range(n_eps)\n",
    "    ]\n",
    "    avg_reward = np.mean(rewards)\n",
    "    return -avg_reward\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Launch Bayesian optimization\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "res_bo = gp_minimize(\n",
    "    func            = bo_objective,\n",
    "    dimensions      = space,\n",
    "    acq_func        = \"EI\",             # Expected Improvement\n",
    "    n_calls         = 50,               # total evaluations\n",
    "    n_random_starts = 10,               # initial random θ’s\n",
    "    callback        = [VerboseCallback(n_total=50)],\n",
    "    random_state    = 0\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Extract the best θ and its corresponding average reward\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "best_theta  = np.array(res_bo.x, dtype=np.float32)\n",
    "best_reward = -res_bo.fun\n",
    "\n",
    "print(f\"Bayesian Opt complete → best avg reward ≈ {best_reward:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35396783-ee10-4c80-be02-f1129889a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"theta2.npy\", best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73176986-1922-4c10-8435-cbf658ff93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Recreate the per‐customer policy extraction function\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def get_policy_actions(df: pd.DataFrame, theta: np.ndarray) -> pd.DataFrame:\n",
    "    heads = unpack_theta(theta)\n",
    "    X = df[feat_cols].to_numpy(dtype=np.float32)  # (M, P)\n",
    "\n",
    "    # discount\n",
    "    disc_lin  = X.dot(heads['disc_w']) + heads['disc_b']\n",
    "    discounts = expit(disc_lin)\n",
    "\n",
    "    # PhoneService\n",
    "    ps_lin = X.dot(heads['ps_w']) + heads['ps_b']\n",
    "    p_ps   = expit(ps_lin)\n",
    "    phone  = (p_ps > 0.5).astype(int)\n",
    "\n",
    "    # InternetService (softmax over 3)\n",
    "    int_lin = X.dot(heads['int_w'].T) + heads['int_b'][None, :]\n",
    "    int_prob= softmax(int_lin, axis=1)\n",
    "    choice  = np.argmax(int_prob, axis=1)\n",
    "    dsl     = (choice == 1).astype(int)\n",
    "    fiber   = (choice == 2).astype(int)\n",
    "\n",
    "    # Six yes/no pairs\n",
    "    toggles = {}\n",
    "    for i, name in enumerate(pair_names):\n",
    "        lin = X.dot(heads['pair_w'][i]) + heads['pair_b'][i]\n",
    "        p   = expit(lin)\n",
    "        yes = (p > 0.5).astype(int)\n",
    "        no  = 1 - yes\n",
    "        toggles[f'{name}_Yes'] = yes\n",
    "        toggles[f'{name}_No']  = no\n",
    "\n",
    "    # Assemble into a DataFrame\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out['Discount']                   = discounts\n",
    "    out['PhoneService']               = phone\n",
    "    out['InternetService_DSL']        = dsl\n",
    "    out['InternetService_Fiber optic']= fiber\n",
    "    for col, arr in toggles.items():\n",
    "        out[col] = arr\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Usage: compute and view the policy for your current cus_base\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "cus_base = processed_df[processed_df[\"Churn\"] == 1]\n",
    "\n",
    "def get_average_action(df: pd.DataFrame, theta: np.ndarray) -> pd.Series:\n",
    "    avg_df = df[feat_cols].mean().to_frame().T\n",
    "    return get_policy_actions(avg_df, theta).iloc[0]\n",
    "\n",
    "avg_action = get_average_action(cus_base, best_theta)\n",
    "print(avg_action)\n",
    "\n",
    "\n",
    "actions = get_policy_actions(cus_base, best_theta)\n",
    "\n",
    "# Summary across all customers\n",
    "summary = actions.mean().round(3)\n",
    "print(\"\\nPolicy summary (means across all customers):\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae8c35-4722-49dc-a0c0-b61646aac5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
