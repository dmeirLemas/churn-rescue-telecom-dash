# ğŸ“„ Project Documentation: Churn Rescue & Retention Optimization

KAGGLE NOTEBOOK: https://www.kaggle.com/code/demirelmas/hacklab-sim

## OR YOU COULD JUST GO TO THE WEBSITE TO VIEW THE DASHBOARD

DASHBOARD URL: https://hacklab6foot6.streamlit.app/

## ğŸ“ Project Structure

```
project_root/
â”‚
â”œâ”€â”€ app.py                     # Main Streamlit dashboard app
â”œâ”€â”€ churn_utils/              # All Python helper modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retention.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â”œâ”€â”€ processing.py
â”‚   â””â”€â”€ ... (others)
â”œâ”€â”€ theta.npy                 # Multi-head retention weights
â”œâ”€â”€ theta_rename.npy          # Additional head (for services or internet)
â”œâ”€â”€ kmeans.pkl                # Pretrained KMeans clustering model
â”œâ”€â”€ lr_model.pkl              # Trained LogisticRegression model
â”œâ”€â”€ nn_churn.pkl              # Trained NearestNeighbors churn model
â”œâ”€â”€ btUTgX.xlsx               # Main customer data
â”œâ”€â”€ newdataset2.csv           # Complaint dataset
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                 # â† (this file)
```

---

## ğŸ› ï¸ Requirements

Create a virtual environment and install dependencies:

```bash
python3 -m venv env
source env/bin/activate  # or .\env\Scripts\activate on Windows

pip install -r requirements.txt
```

### `requirements.txt` contents:

```txt
streamlit
pandas
numpy
scikit-learn
xgboost
seaborn
matplotlib
imblearn
joblib
scipy
nltk
openpyxl
```

> ğŸ”¹ *Note:* For sentiment scoring in complaints, `nltk` and `vaderSentiment` (or `nltk.sentiment.vader`) are required. Run:
```bash
python -m nltk.downloader vader_lexicon
```

---

## ğŸš€ Running the App

From the root directory:

```bash
streamlit run app.py
```

Your dashboard will launch in the browser with multiple pages:
- ğŸ—‚ï¸ **Data** â€” View raw and processed samples, feature distributions
- ğŸ¤– **Model** â€” View LR model coefficients and performance (precision/recall/F1/AUC)
- ğŸ§ª **Retention Simulator** â€” Apply and compare strategies
- ğŸ“Š **Churn Insights** â€” Service counts vs. churn, cluster breakdowns
- ğŸ“ˆ **Profit Simulator** â€” Revenue and cost analysis under different strategies

---

## ğŸ“Œ Notes on Key Components

### 1. **Retention Models**
- **Multi-head model** (`theta.npy`) and (`theta_rename.npy`) applies discounts, services, and internet changes.
- **Contract-only model** (`theta_contract.npy`) changes contract type based on tenure and eligibility.
- A **merged model** applies both in sequence.

### 2. **Key Retention Rules**
- Contract changes only apply if:
  - Tenure â‰¥ 4 months
  - No downgrade is allowed
- Services are turned on if Internet is available
- Dropping fiber reduces MonthlyCharges by a one-time 25 unit

### 3. **Data Loading**
The main dataset is `btUTgX.xlsx` and complaints are in `newdataset2.csv`. These are merged by `customerID`.

### 4. **Training and Evaluation**
- `lr_model.pkl` is trained.
- Confusion matrix and classification reports are shown in the dashboard.

---


## âœ… Final Checklist Before Running

- [x] Activate virtual environment  
- [x] Run `streamlit run app.py`  
- [x] Ensure `theta.npy`, `theta_rename.npy`, `theta.npy`, and models are in root  
- [x] Confirm that `kmeans.pkl`, `lr_model.pkl`, `nn_churn.pkl` exist  
- [x] Data files (`btUTgX.xlsx` & `newdataset2.csv`) are in root  

---

